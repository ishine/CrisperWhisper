{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f1425a-6cf1-4af3-a84f-321aef60da3b",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47764c8e-29dc-448e-a95c-54c700903d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import tqdm\n",
    "\n",
    "from utils import (\n",
    "    add_gaussian_noise_to_audio,\n",
    "    add_score_metrics,\n",
    "    adjust_pauses,\n",
    "    common_indexes,\n",
    "    configure_model,\n",
    "    configure_model_generation,\n",
    "    find_matching_indexes,\n",
    "    get_alignment_head_generator,\n",
    "    get_processor_config_genConfig,\n",
    "    get_mixing_scale,\n",
    "    load_labels,\n",
    "    prepare_ground_truth,\n",
    "    process_audio_files,\n",
    "    remove_punctuation,\n",
    "    setup_pipeline,\n",
    "    top_heads_by_f1\n",
    ")\n",
    "from pathlib import Path\n",
    "from evaluate_word_segmentation import (\n",
    "    convert_timestamps_from_labels_json_to_TimestampedOutput,\n",
    "    convert_timestamps_from_transformers_pipe_to_TimestampedOutput,\n",
    "    batch_evaluate_segmentation,\n",
    ")\n",
    "from crisper_whisper import (\n",
    "    WhisperForConditionalGenerationWithAttentionLoss\n",
    ")\n",
    "\n",
    "from speech_recognition import WhisperXModel, WhisperTimestamped, ModelConfig, transcribe_speech_files\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoProcessor, \n",
    "    WhisperProcessor, \n",
    "    AutoModelForSpeechSeq2Seq, \n",
    "    GenerationConfig, \n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from typing import Dict, Generator, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf3054-4426-4d3c-9c63-aa499eabeff3",
   "metadata": {},
   "source": [
    "## Set a Experiment Configuration Documentation\n",
    "\n",
    "This document outlines the parameters used in the configuration for Whisper models used in the experiments. Each key in the `experiment_config` dictionary holds specific experimental settings as detailed below:\n",
    "\n",
    "### Models\n",
    "- **`models`**: List of transformer Whisper models to be evaluated. Each entry is a dictionary specifying the model's name and path e.g.:\n",
    "  - **`name`**: \"large-v3\" â€” points to a pre-trained model at **`path`**: \"openai/whisper-large-v3\".\n",
    "\n",
    "\n",
    "### Output Path\n",
    "- **`output_path`**: Path where experiment outputs are stored, set to `\"interspeech_2024/timing_outputs_head_selection/\"`.\n",
    "\n",
    "### Dataset Configuration\n",
    "- **`dataset_base_path`**: Base path to the datasets used in experiments, `/home/azureuser/data/english_speech`.\n",
    "\n",
    "### Head Selection Configuration\n",
    "- **`head_selection_dataset`**: The dataset used for head selection, specifically `\"timit\"` is chosen for its good timestamp quality.\n",
    "- **`head_selection_collar`**: The collar size in seconds for the head selection phase, set to `0.1`.\n",
    "- **`head_selection_num_samples`**: Number of samples used in the head selection phase, `100`.\n",
    "\n",
    "### Experiment Parameters\n",
    "- **`experiment_num_samples`**: Number of samples to use in each of the datasets for the main experiments, `500`.\n",
    "- **`test_datasets`**: List of datasets to be used in experiments: `[\"timit\", \"ami_hf\", \"cv_14\"]`.\n",
    "- **`median_filter_widths`**: List of widths for the median filter applied in processing the DTW Matrix.\n",
    "- **`num_heads`**: Number of greedily picked best performing heads to test.\n",
    "- **`add_noise`**: Specifies which noise configurations should be tested. `[True]` means with noise shall be tested  and `[False]` means without noise shall be tested. Botch configurations should be testes `[True, False]`.\n",
    "- **`transcripts_must_match`**: Whether the transcripts need to match exactly in the evaluation for timestamps, set to `True`.\n",
    "- **`pause_thresholds`**: Numpy array of pause threshold values to test for splitting artefact pauses.\n",
    "- **`collars`**: List of collar values in seconds for caluclating precision, recall and f1 scores.\n",
    "\n",
    "### Additional Settings\n",
    "- **`device`**: The device where the model is placed for the experiments: `\"cuda:0\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96ee2760-70d1-4108-bfdf-e718ae544fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    \"models\": [\n",
    "        {\"name\": \"large-v3\", \"path\": \"openai/whisper-large-v3\"},\n",
    "        {\"name\": \"large-v2\", \"path\": \"openai/whisper-large-v2\"},\n",
    "        {\"name\": \"crisperWhisper++\", \"path\": \"/home/azureuser/laurin/code/research/output/crisper_whisper_timestamp_finetuned\"}\n",
    "    ],\n",
    "    \"output_path\": \"experiments\",\n",
    "    \"dataset_base_path\": \"/home/azureuser/data/english_speech\",\n",
    "    \"head_selection_dataset\": \"timit\",\n",
    "    \"head_selection_collar\": .1,\n",
    "    \"head_selection_num_samples\": 100,\n",
    "    \"experiment_num_samples\": 500,\n",
    "    \"test_datasets\": [\"timit\", \"ami_hf\", \"cv_14\", \"synthetic_no_fillers_long_pauses\"],\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"median_filter_widths\": [1,3,5,7,9],\n",
    "    \"num_heads\": list(range(1,15)),\n",
    "    \"add_noise\": [True,False],\n",
    "    \"transcripts_must_match\": True,\n",
    "    \"pause_thresholds\": np.linspace(0, 0.2, 20),\n",
    "    \"collars\":[float(x) / 20 for x in range(21)]\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(experiment_config['output_path'], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdd02c-d263-4284-a16e-4deea57c7f69",
   "metadata": {},
   "source": [
    "## Get F1 scores and other metrics for Models defined in the Experiment Configuration\n",
    "\n",
    "In this section, we extract the F1 segmentation Scores on a subset of a dataset for each individiual head of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f019ec68-66af-4510-9570-d56651d53eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing large-v3: 18it [13:30, 45.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m audio_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(experiment_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_base_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m                             experiment_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead_selection_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m                             label[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_audio_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masr_pipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     predicted_transcripts_and_timestamps \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m         (\n\u001b[1;32m     24\u001b[0m             prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions\n\u001b[1;32m     28\u001b[0m     ]\n\u001b[1;32m     30\u001b[0m     new_predictions \u001b[38;5;241m=\u001b[39m adjust_pauses(predicted_transcripts_and_timestamps)\n",
      "File \u001b[0;32m~/laurin/code/crisper_whisper/run_experiments/utils.py:127\u001b[0m, in \u001b[0;36mprocess_audio_files\u001b[0;34m(audio_paths, pipe)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_audio_files\u001b[39m(audio_paths, pipe):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:292\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    231\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    233\u001b[0m ):\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/base.py:1143\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1140\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1141\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1142\u001b[0m     )\n\u001b[0;32m-> 1143\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1067\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1068\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:507\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline._forward\u001b[0;34m(self, model_inputs, return_timestamps, generate_kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m encoder(inputs, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m--> 507\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_whisper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    512\u001b[0m     out \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:568\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    558\u001b[0m         input_features,\n\u001b[1;32m    559\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mreturn_token_timestamps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(generation_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignment_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 568\u001b[0m         outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_token_timestamps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malignment_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_frames\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# 6. Else we're in longform mode which is more complex.\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# We need to chunk the audio input depending on when the model generates timestamp tokens\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \n\u001b[1;32m    577\u001b[0m \u001b[38;5;66;03m# 6.1 Set and retrieve global longform generation variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:207\u001b[0m, in \u001b[0;36mWhisperGenerationMixin._extract_token_timestamps\u001b[0;34m(self, generate_outputs, alignment_heads, time_precision, num_frames)\u001b[0m\n\u001b[1;32m    199\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m timestamps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_frames \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# two cases:\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# 1. num_frames is the same for each sample -> compute the DTW matrix for each sample in parallel\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# 2. num_frames is different, compute the DTW matrix for each sample sequentially\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# we're using np.unique because num_frames can be int/list/tuple\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;66;03m# if num_frames is the same, no need to recompute matrix, std and mean for each element of the batch\u001b[39;00m\n\u001b[1;32m    209\u001b[0m         num_frames \u001b[38;5;241m=\u001b[39m num_frames \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_frames, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m num_frames[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    211\u001b[0m         weights \u001b[38;5;241m=\u001b[39m weights[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : num_frames \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/numpy/lib/arraysetops.py:272\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_unique_dispatcher)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(ar, return_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m            return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, equal_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Find the unique elements of an array.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     ar \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m         ret \u001b[38;5;241m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                         equal_nan\u001b[38;5;241m=\u001b[39mequal_nan)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "for model_config in experiment_config['models']:\n",
    "    model_name = model_config['name']\n",
    "    model_path = model_config['path']\n",
    "    model = configure_model(model_path, experiment_config['device'])\n",
    "    processor, config, generation_config = get_processor_config_genConfig(model_path)\n",
    "    alignment_head_generator = get_alignment_head_generator(32, 20)\n",
    "    \n",
    "    for alignment_heads in tqdm.tqdm(alignment_head_generator, desc=f'Processing {model_name}'):\n",
    "        configure_model_generation(model, model_name, processor, alignment_heads, 7)\n",
    "        labels = load_labels(experiment_config['dataset_base_path'], experiment_config['head_selection_dataset'],'test', experiment_config['head_selection_num_samples'])\n",
    "        ground_truth_transcripts_and_timestamps = prepare_ground_truth(labels)\n",
    "        asr_pipeline = setup_pipeline(model, processor, experiment_config['device'])\n",
    "        \n",
    "        audio_paths = [os.path.join(experiment_config['dataset_base_path'],\n",
    "                                    experiment_config['head_selection_dataset'],\n",
    "                                    label['audio']) for label in labels]\n",
    "        try:\n",
    "            predictions = process_audio_files(audio_paths, asr_pipeline)\n",
    "        \n",
    "            predicted_transcripts_and_timestamps = [\n",
    "                (\n",
    "                    prediction[\"text\"],\n",
    "                    convert_timestamps_from_transformers_pipe_to_TimestampedOutput(prediction[\"chunks\"])\n",
    "                )\n",
    "                for prediction in predictions\n",
    "            ]\n",
    "            \n",
    "            new_predictions = adjust_pauses(predicted_transcripts_and_timestamps)\n",
    "            \n",
    "            seg_metrics, _ = batch_evaluate_segmentation(\n",
    "                ground_truth_transcripts_and_timestamps,\n",
    "                new_predictions,\n",
    "                collar=experiment_config['head_selection_collar'],\n",
    "                transcripts_must_match=experiment_config['transcripts_must_match']\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                'heads': alignment_heads, \n",
    "                f'F1_collar.{experiment_config[\"head_selection_collar\"]}': seg_metrics.f1_score,\n",
    "                f'avg_iou_collar.{experiment_config[\"head_selection_collar\"]}': seg_metrics.avg_iou\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "\n",
    "with open(os.path.join(experiment_config['output_path'], 'head_results.json'), 'w') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37949cee-639f-49b3-bf0c-f8aa6277fe4f",
   "metadata": {},
   "source": [
    "## Calculate results to run ablation studies on heads\n",
    "\n",
    "In this section, we will calculate the results when greedily adding ,,the best\" heads from the previous run. Further we will vary various hyperparameters that might have a influence on the segmentation so we are able to perform ablation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e589b143-c2fe-4c19-8695-77e0c00d0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_config['output_path'],'head_results.json'), 'r') as f:\n",
    "    result = json.load(f)\n",
    "head_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c769d6-1fcd-4166-b3c8-ed74387f708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on {'name': 'large-v3', 'path': 'openai/whisper-large-v3'}\n",
      "working on timit\n"
     ]
    }
   ],
   "source": [
    "# Set logging level to CRITICAL to suppress all other log messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "results = []\n",
    "output = []\n",
    "for model_config in tqdm.tqdm(experiment_config['models']):\n",
    "    print(f'working on {model_config}')\n",
    "    model_name = model_config['name']\n",
    "    model_path = model_config['path']\n",
    "    model = configure_model(model_path, experiment_config[\"device\"])\n",
    "    processor, config, generation_config = get_processor_config_genConfig(model_path)\n",
    " \n",
    "    for dataset in experiment_config[\"test_datasets\"]:\n",
    "        print(f'working on {dataset}')\n",
    "        for add_noise in experiment_config[\"add_noise\"]:\n",
    "            for median_filter_width in experiment_config[\"median_filter_widths\"]:\n",
    "                for num_heads in experiment_config[\"num_heads\"]:\n",
    "                    top_heads_dict = top_heads_by_f1(head_df, num_heads, experiment_config[\"head_selection_collar\"])\n",
    "                    alignment_heads = top_heads_dict[model_name]\n",
    "                    configure_model_generation(model, model_name, processor, alignment_heads, median_filter_width)\n",
    "                    labels = load_labels(experiment_config[\"dataset_base_path\"],\n",
    "                                         dataset,\n",
    "                                         split='test',\n",
    "                                         limit=experiment_config[\"experiment_num_samples\"])\n",
    "                    ground_truth_transcripts_and_timestamps = prepare_ground_truth(labels)\n",
    "                    asr_pipeline = setup_pipeline(model, processor, experiment_config[\"device\"])\n",
    "                    audio_paths = [os.path.join(experiment_config[\"dataset_base_path\"], dataset, label['audio']) for label in labels]\n",
    "                    if add_noise:\n",
    "                        audio_paths = [add_gaussian_noise_to_audio(path, random.randint(1,8)) for path in audio_paths]\n",
    "                    \n",
    "                    predictions = process_audio_files(audio_paths, asr_pipeline)\n",
    "                    \n",
    "                    predicted_transcripts_and_timestamps = [\n",
    "                        (\n",
    "                            prediction[\"text\"],\n",
    "                            convert_timestamps_from_transformers_pipe_to_TimestampedOutput(prediction[\"chunks\"])\n",
    "                        )\n",
    "                        for prediction in predictions\n",
    "                    ]\n",
    "                    for threshold in experiment_config[\"pause_thresholds\"]:\n",
    "                        new_predictions = adjust_pauses(predicted_transcripts_and_timestamps, threshold=threshold)          \n",
    "                        for collar in experiment_config[\"collars\"]:\n",
    "                            seg_metrics, seg_metrics_list = batch_evaluate_segmentation(\n",
    "                                ground_truth_transcripts_and_timestamps,\n",
    "                                new_predictions,\n",
    "                                collar=collar,\n",
    "                                transcripts_must_match=experiment_config[\"transcripts_must_match\"]\n",
    "                            )\n",
    "                            addendum = '_noise' if add_noise else ''\n",
    "                            output.append({\n",
    "                                'Threshold': threshold,\n",
    "                                'Collar': collar,\n",
    "                                'Model': f'{model_name}',\n",
    "                                'MedianFilterWidth' : median_filter_width,\n",
    "                                'Dataset': dataset + addendum,\n",
    "                                'Recall': seg_metrics.recall,\n",
    "                                'Precision': seg_metrics.precision,\n",
    "                                'F1 Score': seg_metrics.f1_score,\n",
    "                                'Avg IOU': seg_metrics.avg_iou,\n",
    "                                'num_heads': num_heads,\n",
    "                                'predictions' : new_predictions,\n",
    "                                'gts': ground_truth_transcripts_and_timestamps,\n",
    "                                'metrics': seg_metrics_list\n",
    "                            })\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fa925-754e-4bc1-afc2-aec1a2d8eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_config['output_path'], 'full_ablations.pickle'), 'wb') as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ade8c-4848-4af6-bdaa-8a3f2c7bc836",
   "metadata": {},
   "source": [
    "## Optional: Get Baseline Performance of WhisperTimestamped and WhisperX\n",
    "\n",
    "In this section, we can additionally compute the baseline performance for:\n",
    "\n",
    "- **WhisperTimestamped:** [GitHub Repository](https://github.com/linto-ai/whisper-timestamped/)\n",
    "- **WhisperX:** [GitHub Repository](https://github.com/m-bain/whisperX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ce7f6-87bf-4261-985b-da0a3c2fc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output=[]\n",
    "for i,model_config in enumerate([ModelConfig.TIMESTAMPED_WHISPER, ModelConfig.WHISPER_X]):\n",
    "    if i == 1:\n",
    "        model = WhisperXModel(**model_config)\n",
    "    elif i == 0:\n",
    "        model = WhisperTimestamped(**model_config)\n",
    "    for dataset in experiment_config[\"test_datasets\"]:\n",
    "        for add_noise in experiment_config[\"add_noise\"]:\n",
    "            labels = load_labels(experiment_config[\"dataset_base_path\"],\n",
    "                                 dataset,\n",
    "                                 split='test',\n",
    "                                 limit=experiment_config[\"experiment_num_samples\"])\n",
    "            ground_truth_transcripts_and_timestamps = prepare_ground_truth(labels)\n",
    "            audio_paths = [os.path.join(experiment_config[\"dataset_base_path\"], dataset, label['audio']) for label in labels]\n",
    "            if add_noise:\n",
    "                audio_paths = [add_gaussian_noise_to_audio(path, random.randint(1,8)) for path in audio_paths]\n",
    "                        \n",
    "            outputs, error_ids = transcribe_speech_files(model=model, dataset_name=dataset, audio_paths=audio_paths)\n",
    "            timestamped_outputs = [(output.prediction_str,output.timestamped_outputs) for output in outputs]\n",
    "            for collar in experiment_config[\"collars\"]:\n",
    "                seg_metrics, seg_metrics_list = batch_evaluate_segmentation(\n",
    "                    ground_truth_transcripts_and_timestamps,\n",
    "                    timestamped_outputs,\n",
    "                    collar=collar,\n",
    "                    transcripts_must_match=experiment_config[\"transcripts_must_match\"]\n",
    "                )\n",
    "                addendum = '_noise' if add_noise else ''\n",
    "                baseline_output.append({\n",
    "                    'Threshold': None,\n",
    "                    'Collar': collar,\n",
    "                    'Model': 'WhisperX' if i==1 else 'WhisperTimestamped',\n",
    "                    'MedianFilterWidth' : None,\n",
    "                    'Dataset': dataset + addendum,\n",
    "                    'Recall': seg_metrics.recall,\n",
    "                    'Precision': seg_metrics.precision,\n",
    "                    'F1 Score': seg_metrics.f1_score,\n",
    "                    'Avg IOU': seg_metrics.avg_iou,\n",
    "                    'num_heads': None,\n",
    "                    'predictions' : outputs,\n",
    "                    'gts': ground_truth_transcripts_and_timestamps,\n",
    "                    'metrics': seg_metrics_list\n",
    "                })\n",
    "\n",
    "\n",
    "with open(os.path.join(experiment_config['output_path'], 'baseline_ablations.pickle'), 'wb') as f:\n",
    "    pickle.dump(baseline_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70245ef-b510-4b4d-9533-3f9f53957557",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "In this section, we will load the results from the ablation and visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64aac97-2530-48c6-80d5-21ec66c0cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "with open(os.path.join(experiment_config['output_path'], 'performance_vs_num_heads_original_granular.pickle'), 'rb') as f:\n",
    "    output = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630bafa7-2e79-4a1d-bb2e-a10a90f8103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_config = {\n",
    "    \"models_to_compare\": [\"large-v3\",\"crisperWhisper++\"],\n",
    "    \"output_path\": \"interspeech_2024/timing_outputs_head_selection/\",\n",
    "    \"visualization_datasets\": [\"timit\", \"ami_hf\", \"cv_14\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecd2ce-41fd-4b83-ac04-d805ecfa8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a17f19-5adb-4d76-8f15-e023ebb3590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the DataFrame for each dataset\n",
    "df['matching_indexes'] = df.apply(find_matching_indexes, axis=1)\n",
    "# Group by 'Model' and 'Dataset' and apply the common_indexes function\n",
    "common_idx_df = df.groupby(['Model', 'Dataset'])['matching_indexes'].agg(common_indexes).reset_index()\n",
    "\n",
    "# Merge the common indexes back into the original DataFrame\n",
    "df = pd.merge(df, common_idx_df, on=['Model', 'Dataset'], suffixes=('', '_common'))\n",
    "df['F1 Score clean'] = df.apply(lambda row: add_score_metrics(row, 'f1_score'), axis=1)\n",
    "df['Avg IOU clean'] = df.apply(lambda row: add_score_metrics(row, 'avg_iou'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d4056-2a70-42c0-825d-25ea6e8b73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "for dataset in visualization_config[\"visualization_datasets\"]:\n",
    "    collar = .1\n",
    "    threshold = 0.08\n",
    "    filtered_df = df[df['Collar'] == collar]\n",
    "    filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Threshold'] == threshold]\n",
    "    filtered_df = filtered_df[filtered_df['Model'].isin(visualization_config[\"models_to_compare\"])]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    # Set the plotting style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Initialize the matplotlib figure\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Define colors and markers for better distinction\n",
    "    color_f1 = 'tab:blue'\n",
    "    color_iou = 'tab:green'\n",
    "    marker_f1 = 'o'\n",
    "    marker_iou = 'X'\n",
    "    \n",
    "    # Plotting F1 Score on the first y-axis\n",
    "    f1_lines = sns.lineplot(data=filtered_df, x='num_heads', y='F1 Score', hue='Model', marker=marker_f1, ax=ax1, palette='Blues')\n",
    "    ax1.set_xlabel('Number of Heads', fontsize=14)\n",
    "    ax1.set_ylabel('F1 Score', fontsize=14, color=color_f1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color_f1)\n",
    "    ax1.legend(title='Model (F1 Score)', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "    \n",
    "    # Add horizontal line for F1 Score baseline\n",
    "    #ax1.axhline(y=baseline_f1, color='red', linestyle='--', linewidth=2, label='F1 Baseline Heads V2')\n",
    "    ax1.legend(title='F1 Score', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "    \n",
    "    # Create a second y-axis for Avg IOU, sharing the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    iou_lines = sns.lineplot(data=filtered_df, x='num_heads', y='Avg IOU', hue='Model', marker=marker_iou, ax=ax2, palette='Greens')\n",
    "    ax2.set_ylabel('Avg IOU', fontsize=14, color=color_iou)\n",
    "    ax2.tick_params(axis='y', labelcolor=color_iou)\n",
    "    ax2.legend(title='Model (Avg IOU)', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "    ax2.legend(title='Avg IOU', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "    \n",
    "    # Title and layout adjustment\n",
    "    fig.suptitle(f'Comparison of F1 Score and Avg IOU vs. Number of greedily selected Heads at Collar={collar} for Dataset: {dataset}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56165519-3773-49f3-befe-ba5a94145ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Calculate the mean F1 Score and Avg IOU for each Model and num_heads\n",
    "collar = .1\n",
    "filtered_df = df[df['Collar'] == collar]\n",
    "filtered_df = filtered_df[filtered_df['Model'].isin(visualization_config[\"models_to_compare\"])]\n",
    "filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 3]\n",
    "filtered_df = filtered_df[filtered_df['Threshold'] == 0.08]\n",
    "grouped_df = filtered_df.groupby(['Model', 'num_heads']).agg({\n",
    "    'F1 Score': 'mean',\n",
    "    'Avg IOU': 'mean',\n",
    "    'F1 Score': 'mean',\n",
    "    'Avg IOU': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Set the plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Colors for the two metrics\n",
    "colors = ['tab:blue', 'tab:green']\n",
    "\n",
    "# Plot F1 Score\n",
    "sns.lineplot(data=grouped_df, x='num_heads', y='F1 Score', hue='Model', ax=ax1, palette='Blues', marker='o')\n",
    "ax1.set_xlabel('Number of Heads', fontsize=14)\n",
    "ax1.set_ylabel('Average F1 Score', fontsize=14, color=colors[0])\n",
    "ax1.tick_params(axis='y', labelcolor=colors[0])\n",
    "\n",
    "# Create a second y-axis for Avg IOU\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(data=grouped_df, x='num_heads', y='Avg IOU', hue='Model', ax=ax2, palette='Greens', marker='X')\n",
    "ax2.set_ylabel('Average Avg IOU', fontsize=14, color=colors[1])\n",
    "ax2.tick_params(axis='y', labelcolor=colors[1])\n",
    "\n",
    "# Add legends\n",
    "ax1.legend(title='Model (F1 Score)', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "ax2.legend(title='Model (Avg IOU)', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "\n",
    "# Title and layout adjustment\n",
    "fig.suptitle(f'Average F1 Score and Avg IOU vs. Number of Heads Across Datasets for Collar: {collar}', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de54b8-1f2e-45ea-9293-5049fe38eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Calculate the mean F1 Score and Avg IOU for each Model and num_heads\n",
    "for dataset in visualization_config['visualization_datasets']:\n",
    "    # Feel free to play around with these values\n",
    "    collar = .1\n",
    "    filtered_df = df[df['Collar'] == collar]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['num_heads'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Model'].isin(['crisperWhisper++'])]\n",
    "    grouped_df = filtered_df.groupby(['Model', 'Threshold']).agg({\n",
    "        'F1 Score clean': 'mean',\n",
    "        'Avg IOU clean': 'mean',\n",
    "        'F1 Score': 'mean',\n",
    "        'Avg IOU': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Set the plotting style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Initialize the matplotlib figure\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Colors for the two metrics\n",
    "    colors = ['tab:blue', 'tab:green']\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    sns.lineplot(data=grouped_df, x='Threshold', y='F1 Score clean', hue='Model', ax=ax1, palette='Blues', marker='o')\n",
    "    ax1.set_xlabel('Threshold', fontsize=14)\n",
    "    ax1.set_ylabel('Average F1 Score', fontsize=14, color=colors[0])\n",
    "    ax1.tick_params(axis='y', labelcolor=colors[0])\n",
    "    \n",
    "    # Create a second y-axis for Avg IOU\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data=grouped_df, x='Threshold', y='Avg IOU clean', hue='Model', ax=ax2, palette='Greens', marker='X')\n",
    "    ax2.set_ylabel('Average Avg IOU', fontsize=14, color=colors[1])\n",
    "    ax2.tick_params(axis='y', labelcolor=colors[1])\n",
    "    \n",
    "    # Add legends\n",
    "    ax1.legend(title='Model (F1 Score)', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "    ax2.legend(title='Model (Avg IOU)', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "    \n",
    "    # Title and layout adjustment\n",
    "    fig.suptitle(f'Average F1 Score and Avg IOU vs. Threshold for Dataset: {dataset} for Collar: {collar}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251b226-b7b0-4124-84fa-b4711942a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for dataset in visualization_config['visualization_datasets']:\n",
    "    # Define thresholds for each model (customize this as needed)\n",
    "    thresholds = {\n",
    "        'large-v3': 0.08,\n",
    "        'crisperWhisper++': 0.08\n",
    "    }\n",
    "    filtered_df = df[df['Threshold'] == .08]\n",
    "    filtered_df = filtered_df[filtered_df['num_heads'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 1]\n",
    "    fig, ax = plt.subplots()\n",
    "    for model, group in filtered_df.groupby('Model'):\n",
    "        ax.plot(group['Collar'], group['F1 Score'], label=model, marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Collar')\n",
    "    ax.set_ylabel('F1 Score Clean')\n",
    "    ax.set_title(f'F1 Score by Collar for Each Model for Dataset: {dataset}')\n",
    "    ax.legend(title='Model')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e121599-9ca1-4774-a9c3-07590ace6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for dataset in visualization_config['visualization_datasets']:\n",
    "    filtered_df = df[df['Threshold'] ==.08]\n",
    "    filtered_df = filtered_df[filtered_df['num_heads'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    filtered_df = filtered_df[filtered_df['Collar'] == .1]\n",
    "    fig, ax = plt.subplots()\n",
    "    for model, group in filtered_df.groupby('Model'):\n",
    "        ax.plot(group['MedianFilterWidth'], group['F1 Score'], label=model, marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Median Filter Width')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title(f'F1 Score by Collar for Each Model for Dataset: {dataset}')\n",
    "    ax.legend(title='Model')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crisper_whisper_env",
   "language": "python",
   "name": "crisper_whisper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
