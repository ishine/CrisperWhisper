{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f1425a-6cf1-4af3-a84f-321aef60da3b",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47764c8e-29dc-448e-a95c-54c700903d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/.conda/envs/crisper_whisper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/.conda/envs/crisper_whisper/lib/python3.10/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import sys\n",
    "import torch\n",
    "import torchaudio\n",
    "import tqdm\n",
    "\n",
    "from utils import (\n",
    "    add_gaussian_noise_to_audio,\n",
    "    add_score_metrics,\n",
    "    adjust_pauses,\n",
    "    common_indexes,\n",
    "    configure_model,\n",
    "    configure_model_generation,\n",
    "    find_matching_indexes,\n",
    "    get_alignment_head_generator,\n",
    "    get_processor_config_genConfig,\n",
    "    get_mixing_scale,\n",
    "    load_labels,\n",
    "    prepare_ground_truth,\n",
    "    process_audio_files,\n",
    "    remove_punctuation,\n",
    "    setup_pipeline,\n",
    "    top_heads_by_f1\n",
    ")\n",
    "from pathlib import Path\n",
    "from evaluate_word_segmentation import (\n",
    "    convert_timestamps_from_labels_json_to_TimestampedOutput,\n",
    "    convert_timestamps_from_transformers_pipe_to_TimestampedOutput,\n",
    "    batch_evaluate_segmentation,\n",
    ")\n",
    "from crisper_whisper import (\n",
    "    WhisperForConditionalGenerationWithAttentionLoss\n",
    ")\n",
    "\n",
    "from speech_recognition import WhisperXModel, WhisperTimestamped, ModelConfig, transcribe_speech_files\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoProcessor, \n",
    "    WhisperProcessor, \n",
    "    AutoModelForSpeechSeq2Seq, \n",
    "    GenerationConfig, \n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from typing import Dict, Generator, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf3054-4426-4d3c-9c63-aa499eabeff3",
   "metadata": {},
   "source": [
    "## Set a Experiment Configuration Documentation\n",
    "\n",
    "This document outlines the parameters used in the configuration for Whisper models used in the experiments. Each key in the `experiment_config` dictionary holds specific experimental settings as detailed below:\n",
    "\n",
    "### Models\n",
    "- **`models`**: List of transformer Whisper models to be evaluated. Each entry is a dictionary specifying the model's name and path e.g.:\n",
    "  - **`name`**: \"large-v3\" â€” points to a pre-trained model at **`path`**: \"openai/whisper-large-v3\".\n",
    "\n",
    "\n",
    "### Output Path\n",
    "- **`output_path`**: Path where experiment outputs are stored, set to `\"interspeech_2024/timing_outputs_head_selection/\"`.\n",
    "\n",
    "### Dataset Configuration\n",
    "- **`dataset_base_path`**: Base path to the datasets used in experiments, `/home/azureuser/data/english_speech`.\n",
    "\n",
    "### Head Selection Configuration\n",
    "- **`head_selection_dataset`**: The dataset used for head selection, specifically `\"timit\"` is chosen for its good timestamp quality.\n",
    "- **`head_selection_collar`**: The collar size in seconds for the head selection phase, set to `0.1`.\n",
    "- **`head_selection_num_samples`**: Number of samples used in the head selection phase, `100`.\n",
    "\n",
    "### Experiment Parameters\n",
    "- **`experiment_num_samples`**: Number of samples to use in each of the datasets for the main experiments, `500`.\n",
    "- **`test_datasets`**: List of datasets to be used in experiments: `[\"timit\", \"ami_hf\", \"cv_14\"]`.\n",
    "- **`median_filter_widths`**: List of widths for the median filter applied in processing the DTW Matrix.\n",
    "- **`num_heads`**: Number of greedily picked best performing heads to test.\n",
    "- **`add_noise`**: Specifies which noise configurations should be tested. `[True]` means with noise shall be tested  and `[False]` means without noise shall be tested. Botch configurations should be testes `[True, False]`.\n",
    "- **`transcripts_must_match`**: Whether the transcripts need to match exactly in the evaluation for timestamps, set to `True`.\n",
    "- **`pause_thresholds`**: Numpy array of pause threshold values to test for splitting artefact pauses.\n",
    "- **`collars`**: List of collar values in seconds for caluclating precision, recall and f1 scores.\n",
    "\n",
    "### Additional Settings\n",
    "- **`device`**: The device where the model is placed for the experiments: `\"cuda:0\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ee2760-70d1-4108-bfdf-e718ae544fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = {\n",
    "    \"models\": [\n",
    "        {\"name\": \"large-v3\", \"path\": \"openai/whisper-large-v3\"},\n",
    "        {\"name\": \"large-v2\", \"path\": \"openai/whisper-large-v2\"},\n",
    "        {\"name\": \"crisperWhisper++\", \"path\": \"/home/azureuser/laurin/code/research/output/crisper_whisper_timestamp_finetuned\"}\n",
    "    ],\n",
    "    \"output_path\": \"experiments\",\n",
    "    \"dataset_base_path\": \"/home/azureuser/data/english_speech\",\n",
    "    \"head_selection_dataset\": \"timit\",\n",
    "    \"head_selection_collar\": .1,\n",
    "    \"head_selection_num_samples\": 100,\n",
    "    \"experiment_num_samples\": 500,\n",
    "    \"test_datasets\": [\"timit\", \"ami_hf\", \"cv_14\", \"synthetic_no_fillers_long_pauses\"],\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"median_filter_widths\": [1,3,5,7,9],\n",
    "    \"num_heads\": list(range(1,15)),\n",
    "    \"add_noise\": [False],\n",
    "    \"transcripts_must_match\": True,\n",
    "    \"pause_thresholds\": np.linspace(0, 0.2, 5),\n",
    "    \"collars\":[float(x) / 20 for x in range(1,21)]\n",
    "}\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(experiment_config['output_path'], exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bdd02c-d263-4284-a16e-4deea57c7f69",
   "metadata": {},
   "source": [
    "## Get F1 scores and other metrics for Models defined in the Experiment Configuration\n",
    "\n",
    "In this section, we extract the F1 segmentation Scores on a subset of a dataset for each individiual head of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f019ec68-66af-4510-9570-d56651d53eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing large-v3: 0it [00:03, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m audio_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(experiment_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_base_path\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     17\u001b[0m                             experiment_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead_selection_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     18\u001b[0m                             label[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_audio_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masr_pipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     predicted_transcripts_and_timestamps \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m         (\n\u001b[1;32m     24\u001b[0m             prediction[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions\n\u001b[1;32m     28\u001b[0m     ]\n\u001b[1;32m     30\u001b[0m     new_predictions \u001b[38;5;241m=\u001b[39m adjust_pauses(predicted_transcripts_and_timestamps)\n",
      "File \u001b[0;32m~/laurin/code/crisper_whisper/run_experiments/utils.py:127\u001b[0m, in \u001b[0;36mprocess_audio_files\u001b[0;34m(audio_paths, pipe)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_audio_files\u001b[39m(audio_paths, pipe):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_timestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:292\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    231\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    233\u001b[0m ):\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/base.py:1143\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1140\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1141\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1142\u001b[0m     )\n\u001b[0;32m-> 1143\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1067\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1068\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:507\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline._forward\u001b[0;34m(self, model_inputs, return_timestamps, generate_kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m encoder(inputs, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m--> 507\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_whisper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    512\u001b[0m     out \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:557\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m temperature\n\u001b[0;32m--> 557\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mreturn_token_timestamps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(generation_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignment_heads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    568\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_timestamps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_token_timestamps(\n\u001b[1;32m    569\u001b[0m         outputs, generation_config\u001b[38;5;241m.\u001b[39malignment_heads, num_frames\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_frames\n\u001b[1;32m    570\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/generation/utils.py:1479\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1463\u001b[0m         input_ids,\n\u001b[1;32m   1464\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1476\u001b[0m     )\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/generation/utils.py:2340\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2337\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2339\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2340\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/laurin/code/crisper_whisper/run_experiments/crisper_whisper.py:44\u001b[0m, in \u001b[0;36mWhisperForConditionalGenerationWithAttentionLoss.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, attention_mask_for_loss, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, attention_labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     25\u001b[0m     input_features: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m ):\n\u001b[0;32m---> 44\u001b[0m     original_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     cross_attentions \u001b[38;5;241m=\u001b[39m original_output\u001b[38;5;241m.\u001b[39mcross_attentions\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m original_output\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1753\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1749\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1750\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1751\u001b[0m         )\n\u001b[0;32m-> 1753\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1772\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1627\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1621\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1622\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1623\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1624\u001b[0m     )\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1627\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1640\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1443\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1431\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1432\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         use_cache,\n\u001b[1;32m   1441\u001b[0m     )\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1443\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1455\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:869\u001b[0m, in \u001b[0;36mWhisperDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    867\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    877\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:638\u001b[0m, in \u001b[0;36mWhisperSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mor\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;66;03m# TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisperModel is using WhisperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m` when loading the model.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    637\u001b[0m     )\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;66;03m# if key_value_states are provided this layer is used as a cross-attention layer\u001b[39;00m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;66;03m# for the decoder\u001b[39;00m\n\u001b[1;32m    649\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:298\u001b[0m, in \u001b[0;36mWhisperAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    295\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# get key, value proj\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# `past_key_value[0].shape[2] == key_value_states.shape[1]`\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# is checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    304\u001b[0m     is_cross_attention\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m key_value_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    307\u001b[0m ):\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# reuse k,v, cross_attentions\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/crisper_whisper/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "for model_config in experiment_config['models']:\n",
    "    model_name = model_config['name']\n",
    "    model_path = model_config['path']\n",
    "    model = configure_model(model_path, experiment_config['device'])\n",
    "    processor, config, generation_config = get_processor_config_genConfig(model_path)\n",
    "    alignment_head_generator = get_alignment_head_generator(32, 20)\n",
    "    \n",
    "    for alignment_heads in tqdm.tqdm(alignment_head_generator, desc=f'Processing {model_name}'):\n",
    "        configure_model_generation(model, model_name, processor, alignment_heads, 7)\n",
    "        labels = load_labels(experiment_config['dataset_base_path'], experiment_config['head_selection_dataset'],'test', experiment_config['head_selection_num_samples'])\n",
    "        ground_truth_transcripts_and_timestamps = prepare_ground_truth(labels)\n",
    "        asr_pipeline = setup_pipeline(model, processor, experiment_config['device'])\n",
    "        \n",
    "        audio_paths = [os.path.join(experiment_config['dataset_base_path'],\n",
    "                                    experiment_config['head_selection_dataset'],\n",
    "                                    label['audio']) for label in labels]\n",
    "        try:\n",
    "            predictions = process_audio_files(audio_paths, asr_pipeline)\n",
    "        \n",
    "            predicted_transcripts_and_timestamps = [\n",
    "                (\n",
    "                    prediction[\"text\"],\n",
    "                    convert_timestamps_from_transformers_pipe_to_TimestampedOutput(prediction[\"chunks\"])\n",
    "                )\n",
    "                for prediction in predictions\n",
    "            ]\n",
    "            \n",
    "            new_predictions = adjust_pauses(predicted_transcripts_and_timestamps)\n",
    "            \n",
    "            seg_metrics, _ = batch_evaluate_segmentation(\n",
    "                ground_truth_transcripts_and_timestamps,\n",
    "                new_predictions,\n",
    "                collar=experiment_config['head_selection_collar'],\n",
    "                transcripts_must_match=experiment_config['transcripts_must_match']\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'model': model_name,\n",
    "                'heads': alignment_heads, \n",
    "                f'F1_collar.{experiment_config[\"head_selection_collar\"]}': seg_metrics.f1_score,\n",
    "                f'avg_iou_collar.{experiment_config[\"head_selection_collar\"]}': seg_metrics.avg_iou\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "\n",
    "with open(os.path.join(experiment_config['output_path'], 'head_results.json'), 'w') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37949cee-639f-49b3-bf0c-f8aa6277fe4f",
   "metadata": {},
   "source": [
    "## Calculate results to run ablation studies on heads\n",
    "\n",
    "In this section, we will calculate the results when greedily adding ,,the best\" heads from the previous run. Further we will vary various hyperparameters that might have a influence on the segmentation so we are able to perform ablation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e589b143-c2fe-4c19-8695-77e0c00d0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_config['output_path'],'head_results.json'), 'r') as f:\n",
    "    result = json.load(f)\n",
    "head_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c769d6-1fcd-4166-b3c8-ed74387f708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on {'name': 'large-v3', 'path': 'openai/whisper-large-v3'}\n",
      "working on timit\n"
     ]
    }
   ],
   "source": [
    "# Set logging level to CRITICAL to suppress all other log messages\n",
    "logging.getLogger(\"transformers\").setLevel(logging.CRITICAL)\n",
    "results = []\n",
    "output = []\n",
    "for model_config in tqdm.tqdm(experiment_config['models']):\n",
    "    print(f'working on {model_config}')\n",
    "    model_name = model_config['name']\n",
    "    model_path = model_config['path']\n",
    "    model = configure_model(model_path, experiment_config[\"device\"])\n",
    "    processor, config, generation_config = get_processor_config_genConfig(model_path)\n",
    " \n",
    "    for dataset in experiment_config[\"test_datasets\"]:\n",
    "        print(f'working on {dataset}')\n",
    "        for add_noise in experiment_config[\"add_noise\"]:\n",
    "            for median_filter_width in experiment_config[\"median_filter_widths\"]:\n",
    "                for num_heads in experiment_config[\"num_heads\"]:\n",
    "                    top_heads_dict = top_heads_by_f1(head_df, num_heads, experiment_config[\"head_selection_collar\"])\n",
    "                    alignment_heads = top_heads_dict[model_name]\n",
    "                    configure_model_generation(model, model_name, processor, alignment_heads, median_filter_width)\n",
    "                    labels = load_labels(experiment_config[\"dataset_base_path\"],\n",
    "                                         dataset,\n",
    "                                         split='test',\n",
    "                                         limit=experiment_config[\"experiment_num_samples\"])\n",
    "                    ground_truth_transcripts_and_timestamps = prepare_ground_truth(labels)\n",
    "                    asr_pipeline = setup_pipeline(model, processor, experiment_config[\"device\"])\n",
    "                    audio_paths = [os.path.join(experiment_config[\"dataset_base_path\"], dataset, label['audio']) for label in labels]\n",
    "                    if add_noise:\n",
    "                        audio_paths = [add_gaussian_noise_to_audio(path, random.randint(1,8)) for path in audio_paths]\n",
    "                    \n",
    "                    predictions = process_audio_files(audio_paths, asr_pipeline)\n",
    "                    \n",
    "                    predicted_transcripts_and_timestamps = [\n",
    "                        (\n",
    "                            prediction[\"text\"],\n",
    "                            convert_timestamps_from_transformers_pipe_to_TimestampedOutput(prediction[\"chunks\"])\n",
    "                        )\n",
    "                        for prediction in predictions\n",
    "                    ]\n",
    "                    for threshold in experiment_config[\"pause_thresholds\"]:\n",
    "                        new_predictions = adjust_pauses(predicted_transcripts_and_timestamps, threshold=threshold)          \n",
    "                        for collar in experiment_config[\"collars\"]:\n",
    "                            seg_metrics, seg_metrics_list = batch_evaluate_segmentation(\n",
    "                                ground_truth_transcripts_and_timestamps,\n",
    "                                new_predictions,\n",
    "                                collar=collar,\n",
    "                                transcripts_must_match=experiment_config[\"transcripts_must_match\"]\n",
    "                            )\n",
    "                            addendum = '_noise' if add_noise else ''\n",
    "                            output.append({\n",
    "                                'Threshold': threshold,\n",
    "                                'Collar': collar,\n",
    "                                'Model': f'{model_name}',\n",
    "                                'MedianFilterWidth' : median_filter_width,\n",
    "                                'Dataset': dataset + addendum,\n",
    "                                'Recall': seg_metrics.recall,\n",
    "                                'Precision': seg_metrics.precision,\n",
    "                                'F1 Score': seg_metrics.f1_score,\n",
    "                                'Avg IOU': seg_metrics.avg_iou,\n",
    "                                'num_heads': num_heads,\n",
    "                                'predictions' : new_predictions,\n",
    "                                'gts': ground_truth_transcripts_and_timestamps,\n",
    "                                'metrics': seg_metrics_list\n",
    "                            })\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "392dd7b4-a0ba-485b-9a76-030bc13a222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d86521db-a8a5-4f36-b41b-601e2d3d977f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Collar</th>\n",
       "      <th>Model</th>\n",
       "      <th>MedianFilterWidth</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Avg IOU</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>predictions</th>\n",
       "      <th>gts</th>\n",
       "      <th>metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>1</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.007945</td>\n",
       "      <td>0.007938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>1</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.456020</td>\n",
       "      <td>0.456844</td>\n",
       "      <td>0.456432</td>\n",
       "      <td>0.824349</td>\n",
       "      <td>1</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>1</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.702596</td>\n",
       "      <td>0.703864</td>\n",
       "      <td>0.703229</td>\n",
       "      <td>0.768315</td>\n",
       "      <td>1</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>1</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.888248</td>\n",
       "      <td>0.889852</td>\n",
       "      <td>0.889049</td>\n",
       "      <td>0.733277</td>\n",
       "      <td>1</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>1</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.925379</td>\n",
       "      <td>0.927049</td>\n",
       "      <td>0.926213</td>\n",
       "      <td>0.724869</td>\n",
       "      <td>1</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10915</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>3</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.991720</td>\n",
       "      <td>0.992467</td>\n",
       "      <td>0.992093</td>\n",
       "      <td>0.688230</td>\n",
       "      <td>12</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10916</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.85</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>3</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.992473</td>\n",
       "      <td>0.993220</td>\n",
       "      <td>0.992846</td>\n",
       "      <td>0.687960</td>\n",
       "      <td>12</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10917</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.90</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>3</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.993225</td>\n",
       "      <td>0.993974</td>\n",
       "      <td>0.993599</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>12</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10918</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.95</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>3</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.993602</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.687510</td>\n",
       "      <td>12</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10919</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>large-v3</td>\n",
       "      <td>3</td>\n",
       "      <td>timit_noise</td>\n",
       "      <td>0.993602</td>\n",
       "      <td>0.994350</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.687510</td>\n",
       "      <td>12</td>\n",
       "      <td>[( The bungalow was pleasantly situated near t...</td>\n",
       "      <td>[(The bungalow was pleasantly situated near th...</td>\n",
       "      <td>[&lt;evaluate_word_segmentation.PrecisionRecallMe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10920 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Threshold  Collar     Model  MedianFilterWidth      Dataset    Recall  \\\n",
       "0            0.0    0.00  large-v3                  1  timit_noise  0.007931   \n",
       "1            0.0    0.05  large-v3                  1  timit_noise  0.456020   \n",
       "2            0.0    0.10  large-v3                  1  timit_noise  0.702596   \n",
       "3            0.0    0.15  large-v3                  1  timit_noise  0.888248   \n",
       "4            0.0    0.20  large-v3                  1  timit_noise  0.925379   \n",
       "...          ...     ...       ...                ...          ...       ...   \n",
       "10915        0.2    0.80  large-v3                  3  timit_noise  0.991720   \n",
       "10916        0.2    0.85  large-v3                  3  timit_noise  0.992473   \n",
       "10917        0.2    0.90  large-v3                  3  timit_noise  0.993225   \n",
       "10918        0.2    0.95  large-v3                  3  timit_noise  0.993602   \n",
       "10919        0.2    1.00  large-v3                  3  timit_noise  0.993602   \n",
       "\n",
       "       Precision  F1 Score   Avg IOU  num_heads  \\\n",
       "0       0.007945  0.007938  1.000000          1   \n",
       "1       0.456844  0.456432  0.824349          1   \n",
       "2       0.703864  0.703229  0.768315          1   \n",
       "3       0.889852  0.889049  0.733277          1   \n",
       "4       0.927049  0.926213  0.724869          1   \n",
       "...          ...       ...       ...        ...   \n",
       "10915   0.992467  0.992093  0.688230         12   \n",
       "10916   0.993220  0.992846  0.687960         12   \n",
       "10917   0.993974  0.993599  0.687616         12   \n",
       "10918   0.994350  0.993976  0.687510         12   \n",
       "10919   0.994350  0.993976  0.687510         12   \n",
       "\n",
       "                                             predictions  \\\n",
       "0      [( The bungalow was pleasantly situated near t...   \n",
       "1      [( The bungalow was pleasantly situated near t...   \n",
       "2      [( The bungalow was pleasantly situated near t...   \n",
       "3      [( The bungalow was pleasantly situated near t...   \n",
       "4      [( The bungalow was pleasantly situated near t...   \n",
       "...                                                  ...   \n",
       "10915  [( The bungalow was pleasantly situated near t...   \n",
       "10916  [( The bungalow was pleasantly situated near t...   \n",
       "10917  [( The bungalow was pleasantly situated near t...   \n",
       "10918  [( The bungalow was pleasantly situated near t...   \n",
       "10919  [( The bungalow was pleasantly situated near t...   \n",
       "\n",
       "                                                     gts  \\\n",
       "0      [(The bungalow was pleasantly situated near th...   \n",
       "1      [(The bungalow was pleasantly situated near th...   \n",
       "2      [(The bungalow was pleasantly situated near th...   \n",
       "3      [(The bungalow was pleasantly situated near th...   \n",
       "4      [(The bungalow was pleasantly situated near th...   \n",
       "...                                                  ...   \n",
       "10915  [(The bungalow was pleasantly situated near th...   \n",
       "10916  [(The bungalow was pleasantly situated near th...   \n",
       "10917  [(The bungalow was pleasantly situated near th...   \n",
       "10918  [(The bungalow was pleasantly situated near th...   \n",
       "10919  [(The bungalow was pleasantly situated near th...   \n",
       "\n",
       "                                                 metrics  \n",
       "0      [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "1      [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "2      [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "3      [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "4      [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "...                                                  ...  \n",
       "10915  [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "10916  [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "10917  [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "10918  [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "10919  [<evaluate_word_segmentation.PrecisionRecallMe...  \n",
       "\n",
       "[10920 rows x 13 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "694fd695-eae3-4c43-a73e-5731ab6f885d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['timit_noise'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fa925-754e-4bc1-afc2-aec1a2d8eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(experiment_config['output_path'], 'full_ablations.pickle'), 'wb') as f:\n",
    "    pickle.dump(output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ade8c-4848-4af6-bdaa-8a3f2c7bc836",
   "metadata": {},
   "source": [
    "## Optional: Get Baseline Performance of WhisperTimestamped and WhisperX\n",
    "\n",
    "In this section, we can additionally compute the baseline performance for:\n",
    "\n",
    "- **WhisperTimestamped:** [GitHub Repository](https://github.com/linto-ai/whisper-timestamped/)\n",
    "- **WhisperX:** [GitHub Repository](https://github.com/m-bain/whisperX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689ce7f6-87bf-4261-985b-da0a3c2fc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_output=[]\n",
    "for i,model_config in enumerate([ModelConfig.TIMESTAMPED_WHISPER, ModelConfig.WHISPER_X]):\n",
    "    if i == 1:\n",
    "        model = WhisperXModel(**model_config)\n",
    "    elif i == 0:\n",
    "        model = WhisperTimestamped(**model_config)\n",
    "    for dataset in experiment_config[\"test_datasets\"]:\n",
    "        for add_noise in experiment_config[\"add_noise\"]:\n",
    "            labels = load_labels(experiment_config[\"dataset_base_path\"],\n",
    "                                 dataset,\n",
    "                                 split='test',\n",
    "                                 limit=experiment_config[\"experiment_num_samples\"])\n",
    "            ground_truth_transcripts_and_timestamps = prepare_ground_truth(labels)\n",
    "            audio_paths = [os.path.join(experiment_config[\"dataset_base_path\"], dataset, label['audio']) for label in labels]\n",
    "            if add_noise:\n",
    "                audio_paths = [add_gaussian_noise_to_audio(path, random.randint(1,8)) for path in audio_paths]\n",
    "                        \n",
    "            outputs, error_ids = transcribe_speech_files(model=model, dataset_name=dataset, audio_paths=audio_paths)\n",
    "            timestamped_outputs = [(output.prediction_str,output.timestamped_outputs) for output in outputs]\n",
    "            for collar in experiment_config[\"collars\"]:\n",
    "                seg_metrics, seg_metrics_list = batch_evaluate_segmentation(\n",
    "                    ground_truth_transcripts_and_timestamps,\n",
    "                    timestamped_outputs,\n",
    "                    collar=collar,\n",
    "                    transcripts_must_match=experiment_config[\"transcripts_must_match\"]\n",
    "                )\n",
    "                addendum = '_noise' if add_noise else ''\n",
    "                baseline_output.append({\n",
    "                    'Threshold': None,\n",
    "                    'Collar': collar,\n",
    "                    'Model': 'WhisperX' if i==1 else 'WhisperTimestamped',\n",
    "                    'MedianFilterWidth' : None,\n",
    "                    'Dataset': dataset + addendum,\n",
    "                    'Recall': seg_metrics.recall,\n",
    "                    'Precision': seg_metrics.precision,\n",
    "                    'F1 Score': seg_metrics.f1_score,\n",
    "                    'Avg IOU': seg_metrics.avg_iou,\n",
    "                    'num_heads': None,\n",
    "                    'predictions' : outputs,\n",
    "                    'gts': ground_truth_transcripts_and_timestamps,\n",
    "                    'metrics': seg_metrics_list\n",
    "                })\n",
    "\n",
    "\n",
    "with open(os.path.join(experiment_config['output_path'], 'baseline_ablations.pickle'), 'wb') as f:\n",
    "    pickle.dump(baseline_output, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70245ef-b510-4b4d-9533-3f9f53957557",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "In this section, we will load the results from the ablation and visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64aac97-2530-48c6-80d5-21ec66c0cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "with open(os.path.join(experiment_config['output_path'], 'performance_vs_num_heads_original_granular.pickle'), 'rb') as f:\n",
    "    output = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630bafa7-2e79-4a1d-bb2e-a10a90f8103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_config = {\n",
    "    \"models_to_compare\": [\"large-v3\",\"crisperWhisper++\"],\n",
    "    \"output_path\": \"interspeech_2024/timing_outputs_head_selection/\",\n",
    "    \"visualization_datasets\": [\"timit\", \"ami_hf\", \"cv_14\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecd2ce-41fd-4b83-ac04-d805ecfa8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a17f19-5adb-4d76-8f15-e023ebb3590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the DataFrame for each dataset\n",
    "df['matching_indexes'] = df.apply(find_matching_indexes, axis=1)\n",
    "# Group by 'Model' and 'Dataset' and apply the common_indexes function\n",
    "common_idx_df = df.groupby(['Model', 'Dataset'])['matching_indexes'].agg(common_indexes).reset_index()\n",
    "\n",
    "# Merge the common indexes back into the original DataFrame\n",
    "df = pd.merge(df, common_idx_df, on=['Model', 'Dataset'], suffixes=('', '_common'))\n",
    "df['F1 Score clean'] = df.apply(lambda row: add_score_metrics(row, 'f1_score'), axis=1)\n",
    "df['Avg IOU clean'] = df.apply(lambda row: add_score_metrics(row, 'avg_iou'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d4056-2a70-42c0-825d-25ea6e8b73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "for dataset in visualization_config[\"visualization_datasets\"]:\n",
    "    collar = .1\n",
    "    threshold = 0.08\n",
    "    filtered_df = df[df['Collar'] == collar]\n",
    "    filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Threshold'] == threshold]\n",
    "    filtered_df = filtered_df[filtered_df['Model'].isin(visualization_config[\"models_to_compare\"])]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    # Set the plotting style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Initialize the matplotlib figure\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Define colors and markers for better distinction\n",
    "    color_f1 = 'tab:blue'\n",
    "    color_iou = 'tab:green'\n",
    "    marker_f1 = 'o'\n",
    "    marker_iou = 'X'\n",
    "    \n",
    "    # Plotting F1 Score on the first y-axis\n",
    "    f1_lines = sns.lineplot(data=filtered_df, x='num_heads', y='F1 Score', hue='Model', marker=marker_f1, ax=ax1, palette='Blues')\n",
    "    ax1.set_xlabel('Number of Heads', fontsize=14)\n",
    "    ax1.set_ylabel('F1 Score', fontsize=14, color=color_f1)\n",
    "    ax1.tick_params(axis='y', labelcolor=color_f1)\n",
    "    ax1.legend(title='Model (F1 Score)', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "    \n",
    "    # Add horizontal line for F1 Score baseline\n",
    "    #ax1.axhline(y=baseline_f1, color='red', linestyle='--', linewidth=2, label='F1 Baseline Heads V2')\n",
    "    ax1.legend(title='F1 Score', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "    \n",
    "    # Create a second y-axis for Avg IOU, sharing the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    iou_lines = sns.lineplot(data=filtered_df, x='num_heads', y='Avg IOU', hue='Model', marker=marker_iou, ax=ax2, palette='Greens')\n",
    "    ax2.set_ylabel('Avg IOU', fontsize=14, color=color_iou)\n",
    "    ax2.tick_params(axis='y', labelcolor=color_iou)\n",
    "    ax2.legend(title='Model (Avg IOU)', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "    ax2.legend(title='Avg IOU', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "    \n",
    "    # Title and layout adjustment\n",
    "    fig.suptitle(f'Comparison of F1 Score and Avg IOU vs. Number of greedily selected Heads at Collar={collar} for Dataset: {dataset}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56165519-3773-49f3-befe-ba5a94145ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Calculate the mean F1 Score and Avg IOU for each Model and num_heads\n",
    "collar = .1\n",
    "filtered_df = df[df['Collar'] == collar]\n",
    "filtered_df = filtered_df[filtered_df['Model'].isin(visualization_config[\"models_to_compare\"])]\n",
    "filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 3]\n",
    "filtered_df = filtered_df[filtered_df['Threshold'] == 0.08]\n",
    "grouped_df = filtered_df.groupby(['Model', 'num_heads']).agg({\n",
    "    'F1 Score': 'mean',\n",
    "    'Avg IOU': 'mean',\n",
    "    'F1 Score': 'mean',\n",
    "    'Avg IOU': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Set the plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Colors for the two metrics\n",
    "colors = ['tab:blue', 'tab:green']\n",
    "\n",
    "# Plot F1 Score\n",
    "sns.lineplot(data=grouped_df, x='num_heads', y='F1 Score', hue='Model', ax=ax1, palette='Blues', marker='o')\n",
    "ax1.set_xlabel('Number of Heads', fontsize=14)\n",
    "ax1.set_ylabel('Average F1 Score', fontsize=14, color=colors[0])\n",
    "ax1.tick_params(axis='y', labelcolor=colors[0])\n",
    "\n",
    "# Create a second y-axis for Avg IOU\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(data=grouped_df, x='num_heads', y='Avg IOU', hue='Model', ax=ax2, palette='Greens', marker='X')\n",
    "ax2.set_ylabel('Average Avg IOU', fontsize=14, color=colors[1])\n",
    "ax2.tick_params(axis='y', labelcolor=colors[1])\n",
    "\n",
    "# Add legends\n",
    "ax1.legend(title='Model (F1 Score)', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "ax2.legend(title='Model (Avg IOU)', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "\n",
    "# Title and layout adjustment\n",
    "fig.suptitle(f'Average F1 Score and Avg IOU vs. Number of Heads Across Datasets for Collar: {collar}', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de54b8-1f2e-45ea-9293-5049fe38eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Calculate the mean F1 Score and Avg IOU for each Model and num_heads\n",
    "for dataset in visualization_config['visualization_datasets']:\n",
    "    # Feel free to play around with these values\n",
    "    collar = .1\n",
    "    filtered_df = df[df['Collar'] == collar]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['num_heads'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Model'].isin(['crisperWhisper++'])]\n",
    "    grouped_df = filtered_df.groupby(['Model', 'Threshold']).agg({\n",
    "        'F1 Score clean': 'mean',\n",
    "        'Avg IOU clean': 'mean',\n",
    "        'F1 Score': 'mean',\n",
    "        'Avg IOU': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Set the plotting style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Initialize the matplotlib figure\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Colors for the two metrics\n",
    "    colors = ['tab:blue', 'tab:green']\n",
    "    \n",
    "    # Plot F1 Score\n",
    "    sns.lineplot(data=grouped_df, x='Threshold', y='F1 Score clean', hue='Model', ax=ax1, palette='Blues', marker='o')\n",
    "    ax1.set_xlabel('Threshold', fontsize=14)\n",
    "    ax1.set_ylabel('Average F1 Score', fontsize=14, color=colors[0])\n",
    "    ax1.tick_params(axis='y', labelcolor=colors[0])\n",
    "    \n",
    "    # Create a second y-axis for Avg IOU\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data=grouped_df, x='Threshold', y='Avg IOU clean', hue='Model', ax=ax2, palette='Greens', marker='X')\n",
    "    ax2.set_ylabel('Average Avg IOU', fontsize=14, color=colors[1])\n",
    "    ax2.tick_params(axis='y', labelcolor=colors[1])\n",
    "    \n",
    "    # Add legends\n",
    "    ax1.legend(title='Model (F1 Score)', bbox_to_anchor=(1.12, 1), loc='upper left')\n",
    "    ax2.legend(title='Model (Avg IOU)', bbox_to_anchor=(1.12, 0.85), loc='upper left')\n",
    "    \n",
    "    # Title and layout adjustment\n",
    "    fig.suptitle(f'Average F1 Score and Avg IOU vs. Threshold for Dataset: {dataset} for Collar: {collar}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251b226-b7b0-4124-84fa-b4711942a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for dataset in visualization_config['visualization_datasets']:\n",
    "    # Define thresholds for each model (customize this as needed)\n",
    "    thresholds = {\n",
    "        'large-v3': 0.08,\n",
    "        'crisperWhisper++': 0.08\n",
    "    }\n",
    "    filtered_df = df[df['Threshold'] == .08]\n",
    "    filtered_df = filtered_df[filtered_df['num_heads'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    filtered_df = filtered_df[filtered_df['MedianFilterWidth'] == 1]\n",
    "    fig, ax = plt.subplots()\n",
    "    for model, group in filtered_df.groupby('Model'):\n",
    "        ax.plot(group['Collar'], group['F1 Score'], label=model, marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Collar')\n",
    "    ax.set_ylabel('F1 Score Clean')\n",
    "    ax.set_title(f'F1 Score by Collar for Each Model for Dataset: {dataset}')\n",
    "    ax.legend(title='Model')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e121599-9ca1-4774-a9c3-07590ace6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for dataset in visualization_config['visualization_datasets']:\n",
    "    filtered_df = df[df['Threshold'] ==.08]\n",
    "    filtered_df = filtered_df[filtered_df['num_heads'] == 3]\n",
    "    filtered_df = filtered_df[filtered_df['Dataset'] == dataset]\n",
    "    filtered_df = filtered_df[filtered_df['Collar'] == .1]\n",
    "    fig, ax = plt.subplots()\n",
    "    for model, group in filtered_df.groupby('Model'):\n",
    "        ax.plot(group['MedianFilterWidth'], group['F1 Score'], label=model, marker='o')\n",
    "    \n",
    "    ax.set_xlabel('Median Filter Width')\n",
    "    ax.set_ylabel('F1 Score')\n",
    "    ax.set_title(f'F1 Score by Collar for Each Model for Dataset: {dataset}')\n",
    "    ax.legend(title='Model')\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crisper_whisper_env",
   "language": "python",
   "name": "crisper_whisper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
